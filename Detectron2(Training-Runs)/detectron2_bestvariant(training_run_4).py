# -*- coding: utf-8 -*-
"""Detectron2-BestVariant(Training Run:4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwrXH6PYDeAwagk7yU14Qn08UOQQlOyQ
"""

!nvidia-smi
import os
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

"""## Install Detectron2 and dependencies"""

# Install Detectron2
!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'
!pip install roboflow

# Verify installation
import torch, detectron2
!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# COMMON LIBRARIES
import os
import cv2
from datetime import datetime
from google.colab.patches import cv2_imshow

# DATA SET PREPARATION AND LOADING
from detectron2.data.datasets import register_coco_instances
from detectron2.data import DatasetCatalog, MetadataCatalog

# VISUALIZATION
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.visualizer import ColorMode

# CONFIGURATION
from detectron2 import model_zoo
from detectron2.config import get_cfg

# EVALUATION
from detectron2.engine import DefaultPredictor

# TRAINING
from detectron2.engine import DefaultTrainer

"""## OPTION 1: Use Standard Detectron2 Models (Recommended to start)

Available instance segmentation models in Detectron2:
"""

# List all available instance segmentation models
from detectron2.model_zoo import model_zoo

print("Available COCO Instance Segmentation models:")
available_models = [
    "mask_rcnn_R_50_C4_1x",
    "mask_rcnn_R_50_DC5_1x",
    "mask_rcnn_R_50_FPN_1x",
    "mask_rcnn_R_50_C4_3x",
    "mask_rcnn_R_50_DC5_3x",
    "mask_rcnn_R_50_FPN_3x",
    "mask_rcnn_R_101_C4_3x",
    "mask_rcnn_R_101_DC5_3x",
    "mask_rcnn_R_101_FPN_3x",
    "mask_rcnn_X_101_32x8d_FPN_3x"
]

for model in available_models:
    print(f"  - COCO-InstanceSegmentation/{model}.yaml")

"""## Test with Pre-trained Model"""

# Download test image
!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg
image = cv2.imread("./input.jpg")
cv2_imshow(image)

# Configure standard Detectron2 model
cfg = get_cfg()

# Use a standard Detectron2 model (this works!)
CONFIG_NAME = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
cfg.merge_from_file(model_zoo.get_config_file(CONFIG_NAME))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_NAME)

# Set confidence threshold
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

predictor = DefaultPredictor(cfg)
outputs = predictor(image)

print("Predicted classes:", outputs["instances"].pred_classes)
print("Predicted boxes:", outputs["instances"].pred_boxes)

# Visualize results
visualizer = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = visualizer.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

"""## OPTION 2: Install Mask2Former (Optional - Advanced)

If you specifically want Mask2Former, uncomment and run the following:
"""

# Uncomment the following lines if you want to use Mask2Former
# !pip install git+https://github.com/facebookresearch/Mask2Former.git

# # After installation, you can use Mask2Former configs like this:
# # Note: This requires the Mask2Former repository to be installed
# try:
#     from mask2former import add_maskformer2_config
#     cfg_mask2former = get_cfg()
#     add_maskformer2_config(cfg_mask2former)
#     # Now you can use Mask2Former specific configs
#     print("Mask2Former successfully installed!")
# except ImportError:
#     print("Mask2Former not installed. Using standard Detectron2 models.")

"""## Download Custom Dataset"""

# Download your custom dataset
from roboflow import Roboflow
rf = Roboflow(api_key="20RCK2aHcH7vlvsgSE2N")
project = rf.workspace("detectron-pjt6q").project("weld-instance-segmentation")
version = project.version(11)
dataset = version.download("coco-segmentation")

"""## Register Custom Dataset"""

DATA_SET_NAME = dataset.name.replace(" ", "-")
ANNOTATIONS_FILE_NAME = "_annotations.coco.json"

# TRAIN SET
TRAIN_DATA_SET_NAME = f"{DATA_SET_NAME}-train"
TRAIN_DATA_SET_IMAGES_DIR_PATH = os.path.join(dataset.location, "train")
TRAIN_DATA_SET_ANN_FILE_PATH = os.path.join(dataset.location, "train", ANNOTATIONS_FILE_NAME)

register_coco_instances(
    name=TRAIN_DATA_SET_NAME,
    metadata={},
    json_file=TRAIN_DATA_SET_ANN_FILE_PATH,
    image_root=TRAIN_DATA_SET_IMAGES_DIR_PATH
)

# TEST SET
TEST_DATA_SET_NAME = f"{DATA_SET_NAME}-test"
TEST_DATA_SET_IMAGES_DIR_PATH = os.path.join(dataset.location, "test")
TEST_DATA_SET_ANN_FILE_PATH = os.path.join(dataset.location, "test", ANNOTATIONS_FILE_NAME)

register_coco_instances(
    name=TEST_DATA_SET_NAME,
    metadata={},
    json_file=TEST_DATA_SET_ANN_FILE_PATH,
    image_root=TEST_DATA_SET_IMAGES_DIR_PATH
)

# VALID SET
VALID_DATA_SET_NAME = f"{DATA_SET_NAME}-valid"
VALID_DATA_SET_IMAGES_DIR_PATH = os.path.join(dataset.location, "valid")
VALID_DATA_SET_ANN_FILE_PATH = os.path.join(dataset.location, "valid", ANNOTATIONS_FILE_NAME)

register_coco_instances(
    name=VALID_DATA_SET_NAME,
    metadata={},
    json_file=VALID_DATA_SET_ANN_FILE_PATH,
    image_root=VALID_DATA_SET_IMAGES_DIR_PATH
)

# Confirm dataset registration
registered_datasets = [
    data_set
    for data_set in MetadataCatalog.list()
    if data_set.startswith(DATA_SET_NAME)
]
print("Registered datasets:", registered_datasets)

"""## Visualize Dataset"""

metadata = MetadataCatalog.get(TRAIN_DATA_SET_NAME)
dataset_train = DatasetCatalog.get(TRAIN_DATA_SET_NAME)

if len(dataset_train) > 0:
    dataset_entry = dataset_train[0]
    image = cv2.imread(dataset_entry["file_name"])

    visualizer = Visualizer(
        image[:, :, ::-1],
        metadata=metadata,
        scale=0.8,
        instance_mode=ColorMode.IMAGE_BW
    )

    out = visualizer.draw_dataset_dict(dataset_entry)
    cv2_imshow(out.get_image()[:, :, ::-1])
else:
    print("No training data found!")

"""## Train Model Configuration

Choose your model architecture:
"""

# HYPERPARAMETERS - Choose your model
MODEL_CONFIGS = {
    "mask_rcnn_R_50_FPN_3x": {
        "config": "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml",
        "lr": 0.00025,
        "description": "Mask R-CNN with ResNet-50 backbone (good balance)"
    },
    "mask_rcnn_R_101_FPN_3x": {
        "config": "COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml",
        "lr": 0.00025,
        "description": "Mask R-CNN with ResNet-101 backbone (better accuracy)"
    },
    "mask_rcnn_X_101_32x8d_FPN_3x": {
        "config": "COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml",
        "lr": 0.00025,
        "description": "Mask R-CNN with ResNeXt-101 backbone (best accuracy)"
    }
}

# Choose your model here
SELECTED_MODEL = "mask_rcnn_X_101_32x8d_FPN_3x"  # Change this to try different models

ARCHITECTURE = SELECTED_MODEL
CONFIG_FILE_PATH = MODEL_CONFIGS[SELECTED_MODEL]["config"]
BASE_LR = MODEL_CONFIGS[SELECTED_MODEL]["lr"]

print(f"Selected model: {SELECTED_MODEL}")
print(f"Description: {MODEL_CONFIGS[SELECTED_MODEL]['description']}")

# Training parameters
MAX_ITER = 3000
EVAL_PERIOD = 200
NUM_CLASSES = 3  # Update this based on your dataset

# OUTPUT DIR
OUTPUT_DIR_PATH = os.path.join(
    DATA_SET_NAME,
    ARCHITECTURE,
    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
)

os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)

# Configure model
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)

# Dataset configuration
cfg.DATASETS.TRAIN = (TRAIN_DATA_SET_NAME,)
cfg.DATASETS.TEST = (TEST_DATA_SET_NAME,)

# Training configuration
cfg.DATALOADER.NUM_WORKERS = 2
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128

# Learning parameters
cfg.SOLVER.BASE_LR = BASE_LR
cfg.SOLVER.MAX_ITER = MAX_ITER
cfg.TEST.EVAL_PERIOD = EVAL_PERIOD

# Model configuration
cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES
cfg.OUTPUT_DIR = OUTPUT_DIR_PATH

# Additional settings
cfg.INPUT.MASK_FORMAT = 'polygon'  # or 'bitmask'

print("Configuration complete!")
print(f"Training will run for {MAX_ITER} iterations")
print(f"Evaluation every {EVAL_PERIOD} iterations")
print(f"Number of classes: {NUM_CLASSES}")
print(f"Learning rate: {BASE_LR}")
print(f"Output directory: {OUTPUT_DIR_PATH}")

"""## Training"""

trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

# Uncomment to view training curves in tensorboard:
# %load_ext tensorboard
# %tensorboard --logdir $OUTPUT_DIR_PATH

"""## Evaluation"""

# Load trained model
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # Adjust confidence threshold
predictor = DefaultPredictor(cfg)

# Test on validation dataset
dataset_valid = DatasetCatalog.get(VALID_DATA_SET_NAME)

print(f"Testing on {len(dataset_valid)} validation images...")

# Test on first 5 images to avoid overwhelming output
for i, d in enumerate(dataset_valid[:5]):
    print(f"Processing image {i+1}/5...")

    img = cv2.imread(d["file_name"])
    outputs = predictor(img)

    # Check if any instances were detected
    if len(outputs["instances"]) > 0:
        print(f"  - Detected {len(outputs['instances'])} instances")
        print(f"  - Classes: {outputs['instances'].pred_classes.cpu().numpy()}")
        print(f"  - Scores: {outputs['instances'].scores.cpu().numpy()}")
    else:
        print("  - No instances detected")

    visualizer = Visualizer(
        img[:, :, ::-1],
        metadata=metadata,
        scale=0.8,
        instance_mode=ColorMode.IMAGE_BW
    )
    out = visualizer.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])

"""## Model Performance Evaluation (Optional)"""

# Uncomment for comprehensive evaluation
# from detectron2.evaluation import COCOEvaluator, inference_on_dataset
# from detectron2.data import build_detection_test_loader

# evaluator = COCOEvaluator(VALID_DATA_SET_NAME, output_dir=OUTPUT_DIR_PATH)
# val_loader = build_detection_test_loader(cfg, VALID_DATA_SET_NAME)
# results = inference_on_dataset(predictor.model, val_loader, evaluator)
# print("Evaluation results:", results)

"""## Testing on Custom Images"""

# Test on your own images
# custom_image_path = "/path/to/your/image.jpg"
# if os.path.exists(custom_image_path):
#     img = cv2.imread(custom_image_path)
#     outputs = predictor(img)
#
#     visualizer = Visualizer(
#         img[:, :, ::-1],
#         metadata=metadata,
#         scale=0.8,
#         instance_mode=ColorMode.IMAGE_BW
#     )
#     out = visualizer.draw_instance_predictions(outputs["instances"].to("cpu"))
#     cv2_imshow(out.get_image()[:, :, ::-1])

print("Training and evaluation complete!")
print(f"Model saved at: {OUTPUT_DIR_PATH}")
print("\nModel Performance Tips:")
print("1. For better accuracy, try mask_rcnn_R_101_FPN_3x or mask_rcnn_X_101_32x8d_FPN_3x")
print("2. Increase MAX_ITER for longer training (try 5000-10000)")
print("3. Adjust learning rate based on your dataset size")
print("4. Monitor training curves in tensorboard")
print("5. For small datasets, consider data augmentation")

"""## Alternative: Mask2Former Integration (Advanced Users)

If you specifically need Mask2Former, here's how to integrate it:
"""


print("\nTo use Mask2Former:")
print("1. Install: !pip install git+https://github.com/facebookresearch/Mask2Former.git")
print("2. Import: from mask2former import add_maskformer2_config")
print("3. Configure: add_maskformer2_config(cfg)")
print("4. Use Mask2Former config files from their repository")
print("\nMask2Former provides state-of-the-art performance but requires more setup.")
print("The standard Detectron2 models above are excellent for most use cases!")

